---
slug: "/ai-orchestration"
description: "Learn how to orchestrate AI-driven workflows with Conductor, including RAG systems and agentic workflows."
---

# AI Orchestration

Orkes Conductor enables you to efficiently build applications that leverage generative AI models and vector databases with a suite of features:
* **AI System Tasks**—Drag and drop common AI tasks, like storing embeddings or chat completions.
* **AI/LLM and Vector Database Integrations**—Seamlessly connect and access dozens of AI-related integrations in a secure and governed manner.
* **AI Prompt Studio**—Refine, test, and govern prompt templates for AI models.

Some common use cases for AI orchestration include:
* RAG (retrieval augmented generation) systems
* LLM-powered chatbots
* AI classifiers


## Orchestrating AI-powered tasks

Orkes Conductor provides a variety of system AI tasks, which can execute common logic without the need to write code. Depending on the type of tasks, using these tasks may require an AI/LLM integration, a vector database integration, or an AI prompt.


| System AI Task     | Description                        |Prerequisites                 |
|------------------- | ---------------------------------- |----------------------------- |
| [LLM Get Document](/reference-docs/ai-tasks/llm-get-document)  | Retrieve text or JSON content from a URL.                                           | NA                  |
| [LLM Generate Embeddings](/reference-docs/ai-tasks/llm-generate-embeddings) | Generate text embeddings.                                          | <ul><li>Integrate an AI model</li></ul> |
| [LLM Store Embeddings](/reference-docs/ai-tasks/llm-store-embeddings) | Store text embeddings in a vector database.                                   | <ul><li>Integrate an AI model</li> <li>Integrate a vector database</li></ul>            |
| [LLM Index Text](/reference-docs/ai-tasks/llm-index-text)      | Generate and store text embeddings in a vector database.                              | <ul><li>Integrate an AI model</li> <li>Integrate a vector database</li></ul>            |
| [LLM Index Document](/reference-docs/ai-tasks/llm-index-document)  | Chunk, generate, and store text embeddings in a vector database.                    | <ul><li>Integrate an AI model</li> <li>Integrate a vector database</li></ul>            |
| [LLM Get Embeddings](/reference-docs/ai-tasks/llm-get-embeddings) | Retrieve data from a vector database.                                            | <ul><li>Integrate a vector database</li></ul> |
| [LLM Search Index](/reference-docs/ai-tasks/llm-search-index)  | Retrieve data from a vector database based on a search query.                           | <ul><li>Integrate an AI model</li> <li>Integrate a vector database</li></ul>            |
| [LLM Text Complete](/reference-docs/ai-tasks/llm-text-complete) | Generate text from an LLM based on a defined prompt.                                  | <ul><li>Integrate an AI model</li><li>Create an AI prompt</li></ul>                     |
| [LLM Chat Complete](/reference-docs/ai-tasks/llm-chat-complete) | Generate text from an LLM based on a user query and additional system/assistant instructions. | <ul><li>Integrate an AI model</li><li>Create an AI prompt</li></ul> |

If custom logic is required, you can create an external worker instead of using the system AI tasks. To do so, follow the guide on [Using External Workers](developer-guides/using-workers) instead.

Once you have decided which task to use, check out the following guides to begin orchestrating.

## Learn more

```mdx-code-block
import DocCardList from '@theme/DocCardList';

<DocCardList />
```
