---
slug: "/integrations/message-broker/confluent-kafka"
description: "Learn how to integrate Confluent Kafka with your Orkes Conductor cluster."
---

# Confluent Kafka Integration with Orkes Conductor

:::note
The Confluent Kafka configuration is deprecated. For new configurations, use [Apache Kafka](https://orkes.io/content/integrations/message-broker/apache-kafka).
:::

To use the [Event task](https://orkes.io/content/reference-docs/system-tasks/event), [Event Handler](https://orkes.io/content/developer-guides/event-handler), or [enable Change Data Capture (CDC)](https://orkes.io/content/developer-guides/enabling-cdc-on-conductor-workflows) in Orkes Conductor, you must integrate your Conductor cluster with the necessary message brokers. This guide explains how to integrate Confluent Kafka with Orkes Conductor to publish and receive messages from topics. Here’s an overview:

1. Get the required credentials from Confluent Kafka.
2. Configure a new Confluent Kafka integration in Orkes Conductor.
3. Set access limits to the message broker to govern which applications or groups can use them.

## Step 1: Get the Confluent Kafka credentials

To integrate Confluent Kafka with Orkes Conductor, retrieve the following credentials from the Confluent Cloud portal:

- API keys
- Bootstrap server
- Schema registry URL

### Get the API keys

**To retrieve the API keys:**

1. Sign in to the [Confluent Cloud portal](https://confluent.cloud/).
2. Select the Confluent cluster to integrate with Orkes Conductor.
3. Go to **Cluster Overview** > **API Keys** from the left navigation menu.
4. Select **Create Key** > **+ Add key**.
5. Choose either **Global access** or **Granular access**.
6. Copy and store the **Key** and **Secret**.

<p align="center"><img src="/content/img/generating-api-keys-confluent.png" alt="Generating API Keys from Confluent Cloud" width="100%" height="auto"/></p>

### Get the Bootstrap server

**To retrieve the Bootstrap server:**

1. Sign in to the [Confluent Cloud portal](https://confluent.cloud/).
2. Go to **Cluster Overview** > **Cluster Settings** > **Endpoints**.
3. Copy the **Bootstrap server**.

<p align="center"><img src="/content/img/getting-bootstrap-token.png" alt="Getting Bootstrap token from Confluent Cloud" width="100%" height="auto"/></p>

4. Go to **Topics** and identify the **Topic name** to use for this integration.

<p align="center"><img src="/content/img/topics.png" alt="Topics in Confluent Cloud" width="100%" height="auto"/></p>

### Get the Schema registry server

The Schema registry server, API key, and secret are only required if you are integrating with a schema registry (for AVRO protocol).

**To get the Schema registry server and API keys:**

1. Sign in to the [Confluent Cloud portal](https://confluent.cloud/).
2. Go to **Clients** > **Add new client**.
3. In **Copy the configuration snippet for your clients** > **schema.registry.url**, copy the URL.
4. Select **Create Schema Registry API key** to download the file. The downloaded file will have the Schema Registry API key and secret.

<p align="center"><img src="/content/img/schema-registry-url.png" alt="Getting Schema Registry URL" width="100%" height="auto"/></p>

## Step 2: Add an integration for Confluent Kafka

After obtaining the credentials, add a Confluent Kafka integration to your Conductor cluster.

**To create a Confluent Kafka integration:**

1. Go to **Integrations** from the left navigation menu on your Conductor cluster.
2. Select **+ New integration**.
3. In the **Message Broker** section, choose **Confluent Kafka**.
4. Select **+ Add** and enter the following parameters:

| Paremeters | Description | Required / Optional | 
| ---------- | ----------- | ------------------- | 
| Integration name | A name for the integration. | Required. | 
| Bootstrap Server | The bootstrap server of the Confluent Kafka cluster. | Required. | 
| Sending Protocol | The sending protocol for the integration. Supported values:<ul><li>**String**–Sends messages as plain string data.</li><li>**AVRO**–Serializes messages using AVRO. To use a schema registry, select AVRO.</li></ul> | Required. | 
| Connection Security | The security mechanism for connecting to the Kafka cluster. Supported values:<ul><li>**SASL_SSL / PLAIN**–Secure connection using SASL (Simple Authentication and Security Layer) with SSL encryption.</li><li>**SASL_SSL / SCRAM-SHA-256 / JKS**–Secure connection using SASL with SCRAM-SHA-256 authentication and SSL encryption. </li></ul> | Required. | 
| Choose Trust Store file | Upload the Java JKS trust store file with CAs. | Required if **_Connection Security_** is **SASL_SSL / SCRAM-SHA-256 / JKS**. | 
| Trust Store Password | The password for the trust store file. | Required if **_Connection Security_** is **SASL_SSL / SCRAM-SHA-256 / JKS**. | 
| Username | The username to authenticate with the Kafka cluster.<br/>**Note**: For AVRO configuration, use the previously-copied API key as the username. | Required. | 
| Password | The password associated with the username.<br/>**Note**: For AVRO configuration, use the previously-copied API secret as the password. | Required. | 
| Schema Registry URL | The Schema Registry URL copied from the Confluent Kafka console. | Required if **_Sending Protocol_** is **AVRO**. | 
| Schema Registry Auth Type | The authentication mechanism for connecting to the schema registry. Supported values:<ul><li>**Password in URL**</li><li>**Schema Registry User Info (Key/Password)**</li><li>**NONE**</li></ul> | Required if **_Sending Protocol_** is **AVRO**. | 
| Schema Registry API Key | The schema registry API key obtained from the schema registry server. | Required if<ul><li>**_Sending Protocol_** is **AVRO**.</li><li>**_Schema Registry Auth Type_** is **Schema Registry User Info (Key/Password)**.</li></ul> | 
| Schema Registry API Secret | The schema registry API secret obtained from the schema registry server. |  Required if<ul><li>**_Sending Protocol_** is **AVRO**.</li><li>**_Schema Registry Auth Type_** is **Schema Registry User Info (Key/Password)**.</li></ul> | 
| Value Subject Name Strategy | The strategy for constructing the subject name under which the AVRO schema will be registered in the schema registry. Supported values:<ul><li>**io.confluent.kafka.serializers.subject.TopicNameStrategy**</li><li>**io.confluent.kafka.serializers.subject.RecordNameStrategy**</li><li>**io.confluent.kafka.serializers.subject.TopicRecordNameStrategy**</li></ul> | Required if **_Sending Protocol_** is **AVRO**. | 
| Consumer Group ID | The Consumer Group ID from Kafka. This unique identifier helps manage message processing, load balancing, and fault tolerance within consumer groups. | Required. | 
| Description | A description of the integration. | Required. | 

<p align="center"><img src="/content/img/integration-confluent-kafka.png" alt="Confluent Kafka Integration with Orkes Conductor" width="50%" height="auto"/></p>

5. (Optional) Toggle the **Active** button off if you don’t want to activate the integration instantly.
6. Select **Save**.

## Step 3: Set access limits to integration

Once the integration is configured, set access controls to manage which [applications](https://orkes.io/content/access-control-and-security/applications) or [groups](https://orkes.io/content/access-control-and-security/users-and-groups#groups) can use the message broker.

**To provide access to an application or group:**

1. Go to **Access Control** > **Applications** or **Groups** from the left navigation menu on your Conductor cluster.
2. Create a new group/application or select an existing one.
3. In the **Permissions** section, select **+ Add Permission**.
4. In the **Integration** tab, select the required message broker and toggle the necessary permissions.

<p align="center"><img src="/content/img/rbac-confluent-kafka.png" alt="Configuring RBAC for Confluent Kafka Integration" width="70%" height="auto"/></p>

The group or application can now access the message broker according to the configured permissions.

## Next steps

With the integration in place, you can now:

- [Create Event Handlers](https://orkes.io/content/developer-guides/event-handler).
- [Configure Event tasks](https://orkes.io/content/reference-docs/system-tasks/event).
- [Enable Change Data Capture (CDC)](https://orkes.io/content/developer-guides/enabling-cdc-on-conductor-workflows) to send workflow state changes to message brokers.