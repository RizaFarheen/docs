---
slug: "/integrations/message-broker/apache-kafka"
description: "Learn how to integrate Apache Kafka with your Orkes Conductor cluster."
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Apache Kafka Integration with Orkes Conductor

To use the [Event task](https://orkes.io/content/reference-docs/system-tasks/event), [Event Handler](https://orkes.io/content/developer-guides/event-handler), or [enable Change Data Capture (CDC)](https://orkes.io/content/developer-guides/enabling-cdc-on-conductor-workflows) in Orkes Conductor, you must integrate your Conductor cluster with the necessary message brokers. This guide explains how to integrate self-managed Apache Kafka, Amazon MSK, or Confluent Kafka clusters with Orkes Conductor to publish and receive messages from topics. Here’s an overview:

1. Get the required credentials from Kafka.
2. Configure a new Apache Kafka integration in Orkes Conductor.
3. Set access limits to the message broker to govern which applications or groups can use them.

## Step 1: Get the Kafka credentials

To integrate Kafka with Orkes Conductor, retrieve the following credentials from the Kafka cluster:

- API keys
- Bootstrap server
- Schema registry URL 

The configuration steps vary depending on the type of Kafka cluster to be integrated.

<Tabs>
<TabItem title="Get Apache Kafka credentials" value="Get Apache Kafka credentials">

Set up [Apache Kafka](https://kafka.apache.org/documentation/) locally and retrieve the following credentials:

- Bootstrap server
- API key and secret

</TabItem>
<TabItem title="Get Confluent Kafka credentials" value="Get Confluent Kafka credentials">

To integrate Confluent Kafka with Orkes Conductor, retrieve the following credentials from the Confluent Cloud portal:

- API keys
- Bootstrap server
- Schema registry URL

**To retrieve the API keys:**

1. Sign in to the [Confluent Cloud portal](https://confluent.cloud/).
2. Select the Confluent cluster to integrate with Orkes Conductor.
3. Go to **Cluster Overview** > **API Keys** from the left navigation menu.
4. Select **Create Key** > **+ Add key**.
5. Choose either **Global access** or **Granular access**.
6. Copy and store the **Key** and **Secret**.

<p align="center"><img src="/content/img/generating-api-keys-confluent.png" alt="Generating API Keys from Confluent Cloud" width="100%" height="auto"/></p>

**To retrieve the Bootstrap server:**

1. Sign in to the [Confluent Cloud portal](https://confluent.cloud/).
2. Go to **Cluster Overview** > **Cluster Settings** > **Endpoints**.
3. Copy the **Bootstrap server**.

<p align="center"><img src="/content/img/getting-bootstrap-token.png" alt="Getting Bootstrap token from Confluent Cloud" width="100%" height="auto"/></p>

4. Go to **Topics** and identify the **Topic name** to use for this integration.

<p align="center"><img src="/content/img/topics.png" alt="Topics in Confluent Cloud" width="100%" height="auto"/></p>

The Schema registry server, API key, and secret are only required if you are integrating with a schema registry (for AVRO protocol).

**To get the Schema registry server and API keys:**

1. Sign in to the [Confluent Cloud portal](https://confluent.cloud/).
2. Go to **Clients** > **Add new client**.
3. In **Copy the configuration snippet for your clients** > **schema.registry.url**, copy the URL.
4. Select **Create Schema Registry API key** to download the file. The downloaded file will have the Schema Registry API key and secret.

<p align="center"><img src="/content/img/schema-registry-url.png" alt="Getting Schema Registry URL" width="100%" height="auto"/></p>

</TabItem>
<TabItem title="Get Amazon MSK credentials" value="Get Amazon MSK credentials">

To integrate Amazon MSK with Orkes Conductor, retrieve the following credentials from your Amazon MSK console:

- [Bootstrap server](https://docs.aws.amazon.com/msk/latest/developerguide/msk-get-bootstrap-brokers.html)
- Username and Password 
- Consumer Group ID

Refer to the official [Amazon MSK documentation](https://docs.aws.amazon.com/msk/latest/developerguide/getting-started.html) for more details.
</TabItem>
</Tabs>

## Step 2: Add an integration for Apache Kafka

After obtaining the credentials, add an Apache Kafka integration to your Conductor cluster.

**To create an Apache Kafka integration:**

1. Go to **Integrations** from the left navigation menu on your Conductor cluster.
2. Select **+ New integration**.
3. In the **Message Broker** section, choose **Apache Kafka**.
4. Select **+ Add** and enter the following parameters:

| Parameters | Description | Required / Optional | Notes | 
| ---------- | ----------- | ------------------- | ----- | 
| Integration name | A name for the integration. | Required. | – |
| Bootstrap Server | The bootstrap server of the Kafka cluster. | Required. | – |
| Sending Protocol | The sending protocol for the integration. Supported values:<ul><li>**String**–Sends messages as plain string data.</li><li>**AVRO**–Serializes messages using AVRO. To use a schema registry, select AVRO.</li></ul> | Required. | AVRO protocol is not supported for Amazon MSK clusters. | 
| Connection Security | The security mechanism for connecting to the Kafka cluster. Supported values:<ul><li>**SASL_SSL / PLAIN**–Secure connection using SASL (Simple Authentication and Security Layer) with SSL encryption. </li><li>**SASL_SSL / SCRAM-SHA-256 / JKS**–Secure connection using SASL with SCRAM-SHA-256 authentication and SSL encryption.</li><li>**SASL_SSL/SCRAM-SHA-512**–Secure connection using SASL with SCRAM-SHA-512 authentication and SSL encryption.</li><li>**SASL_PLAINTEXT**–Basic authentication mechanism in Kafka without data encryption during transit.</li><li>**PLAINTEXT**–Plain text connection without any encryption or authentication.</li></ul> | Required. | Amazon MSK supports only **SASL_SSL/SCRAM-SHA-512**. | 
| Username | The username to authenticate with the Kafka cluster.<br/>For AVRO configuration, use the API key copied previously as the username. | Required (except for **PLAINTEXT**)| – | 
| Password | The password associated with the username.<br/>For AVRO configuration, use the API secret copied previously as the password. | Required (except for **PLAINTEXT**) | – | 
| Truststore type | If SSL encryption is enabled, select the trust store type. Supported values:<ul><li>**NONE**</li><li>**JKS**–If chosen, upload the Java JKS trust store file with CAs.</li><li>**PEM**–If chosen, upload the PEM certificate file</li></ul>| Required for connection types **SASL_SSL / PLAIN** and **SASL_PLAINTEXT.** | Not supported for Amazon MSK clusters. | 
| Trust Store Password | The password for the trust store. | Required if **Truststore type** is **JKS**. | Not supported for Amazon MSK clusters. | 
| Select Sasl mechanism | The SASL mechanism to connect to the Kafka cluster. Supported values:<ul><li>**SASL_SSL/SCRAM-SHA-512**–Secure connection using SASL with SCRAM-SHA-512 authentication and SSL encryption.</li><li>**PLAIN**– Basic authentication mechanism without encryption, used for non-secure connections.</li></ul> | Required if **_Connection Security_** is **SASL_PLAINTEXT**. | – | 
| Schema Registry URL | The Schema Registry URL copied from the Kafka console. | Required if **_Sending Protocol_** is **AVRO**. | Not supported for Amazon MSK clusters. | 
| Schema Registry Auth Type | The authentication mechanism for connecting to the schema registry. Supported values:<ul><li>**Password in URL**</li><li>**Schema Registry User Info (Key/Password)**</li><li>**NONE**</li></ul> | Required if **_Sending Protocol_** is **AVRO**. | Not supported for Amazon MSK clusters. | 
| Schema Registry API Key | The schema registry API key obtained from the schema registry server. | Required if<ul><li>**_Sending Protocol_** is **AVRO**.</li><li>**_Schema Registry Auth Type_** is **Schema Registry User Info (Key/Password)**.</li></ul> | Not supported for Amazon MSK clusters. | 
| Schema Registry API Secret | The schema registry API secret obtained from the schema registry server. | Required if<ul><li>**_Sending Protocol_** is **AVRO**.</li><li>**_Schema Registry Auth Type_** is **Schema Registry User Info (Key/Password)**.</li></ul> | Not supported for Amazon MSK clusters. | 
| Value Subject Name Strategy | The strategy for constructing the subject name under which the AVRO schema will be registered in the schema registry. Supported values:<ul><li>**io.confluent.kafka.serializers.subject.TopicNameStrategy**</li><li>**io.confluent.kafka.serializers.subject.RecordNameStrategy**</li><li>**io.confluent.kafka.serializers.subject.TopicRecordNameStrategy**</li></ul> | Required if **_Sending Protocol_** is **AVRO**. | Not supported for Amazon MSK clusters. | 
| Consumer Group ID | The Consumer Group ID from Kafka. This unique identifier helps manage message processing, load balancing, and fault tolerance within consumer groups. | Required. | – | 
| Description | A description of the integration. | Required. | – | 

<p align="center"><img src="/content/img/integration-apache-kafka.png" alt="Apache Kafka Integration with Orkes Conductor" width="60%" height="auto"/></p>

5. (Optional) Toggle the **Active** button off if you don’t want to activate the integration instantly.
6. Select **Save**.

## Step 3: Set access limits to integration

Once the integration is configured, set access controls to manage which [applications](https://orkes.io/content/access-control-and-security/applications) or [groups](https://orkes.io/content/access-control-and-security/users-and-groups#groups) can use the message broker.

**To provide access to an application or group:**

1. Go to **Access Control** > **Applications** or **Groups** from the left navigation menu on your Conductor cluster.
2. Create a new group/application or select an existing one.
3. In the **Permissions** section, select **+ Add Permission**.
4. In the **Integration** tab, select the required message broker and toggle the necessary permissions.

<p align="center"><img src="/content/img/rbac-apache-kafka.png" alt="Configuring RBAC for Apache Kafka Integration" width="70%" height="auto"/></p>

The group or application can now access the message broker according to the configured permissions.

## Next steps

With the integration in place, you can now:

- [Create Event Handlers](https://orkes.io/content/developer-guides/event-handler).
- [Configure Event tasks](https://orkes.io/content/reference-docs/system-tasks/event).
- [Enable Change Data Capture (CDC)](https://orkes.io/content/developer-guides/enabling-cdc-on-conductor-workflows) to send workflow state changes to message brokers.